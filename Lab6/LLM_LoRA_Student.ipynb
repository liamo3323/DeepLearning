{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSqI2Yls-22s"
      },
      "source": [
        "# Introduction to Fine-Tuning Large Language Models with LoRA\n",
        "\n",
        "In this laboratory session, we will explore an advanced technique for fine-tuning large language models, specifically focusing on the application of Low-Rank Adaptation (LoRA). This method allows for efficient parameter updating and learning, making it feasible to adapt large-scale models like Facebook's OPT to new tasks without the need for extensive computational resources.\n",
        "\n",
        "#### Objectives:\n",
        "- Understand the principles behind LoRA and its advantages for fine-tuning large language models.\n",
        "- Learn how to apply LoRA to the OPT model, focusing on specific components (LoRA_A and LoRA_B) for adaptation.\n",
        "- Explore the use of Hugging Face's `transformers` and `datasets` libraries for model training and evaluation.\n",
        "- Gain practical experience in handling natural language processing tasks, such as question-answering, through the implementation and fine-tuning of the model.\n",
        "- Evaluate the performance of the fine-tuned model using standard NLP metrics such as BLEU, ROUGE, and Exact Match.\n",
        "\n",
        "#### Prerequisites:\n",
        "Before starting this lab, you should have a basic understanding of PyTorch, neural networks, and NLP concepts. Familiarity with Hugging Face libraries will also be beneficial.\n",
        "\n",
        "#### Lab Overview:\n",
        "The lab is structured as follows:\n",
        "1. **Environment Setup**: We will begin by setting up our working environment, including the installation of necessary libraries and loading of environmental variables.\n",
        "2. **Model Initialization and LoRA Configuration**: You will learn how to initialize the OPT model and configure it using LoRA by modifying specific layers while freezing the rest of the model to prevent unnecessary updates.\n",
        "3. **Data Preparation**: We will load and preprocess the data suitable for our task, creating custom datasets and dataloaders.\n",
        "4. **Fine-Tuning and Evaluation**: You will fine-tune the model on a question-answering task and evaluate its performance using several NLP metrics.\n",
        "5. **Results Analysis**: Finally, we will analyze the results and understand the impact of LoRA fine-tuning on the model's performance.\n",
        "\n",
        "By the end of this lab, you will have hands-on experience fine-tuning large language models using LoRA, which is a valuable skill in the field of artificial intelligence and natural language processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXXcQYsB_brZ"
      },
      "source": [
        "# Import basic libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:04:35.645103Z",
          "iopub.status.busy": "2024-03-10T12:04:35.644757Z",
          "iopub.status.idle": "2024-03-10T12:05:52.940065Z",
          "shell.execute_reply": "2024-03-10T12:05:52.938807Z",
          "shell.execute_reply.started": "2024-03-10T12:04:35.645076Z"
        },
        "id": "uTkHfH0EIR-A",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: no matches found: transformers[torch]\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers[torch]\n",
        "!pip install -q datasets\n",
        "!pip install -q accelerate -U\n",
        "!pip install -q py7zr\n",
        "!pip install -q evaluate nltk rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:05:52.942474Z",
          "iopub.status.busy": "2024-03-10T12:05:52.942133Z",
          "iopub.status.idle": "2024-03-10T12:06:12.255551Z",
          "shell.execute_reply": "2024-03-10T12:06:12.254721Z",
          "shell.execute_reply.started": "2024-03-10T12:05:52.942443Z"
        },
        "id": "yaBv9HTJ_XFF",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/liamo/VScode/DeepLearning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import evaluate\n",
        "from torch import nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlCWhPoq__yZ"
      },
      "source": [
        "# Load Evaluator\n",
        "\n",
        "In this section of the code, we are initializing evaluation metrics for the language model we plan to fine-tune. Specifically, we use the `evaluate` library, which is a part of the Hugging Face ecosystem, designed for evaluating and comparing the performance of models across a wide range of NLP tasks.\n",
        "\n",
        "1. `bleu_scorer = evaluate.load('bleu')`: This line loads the BLEU (Bilingual Evaluation Understudy) scorer from the `evaluate` library. BLEU is a widely used metric for evaluating the quality of text which has been machine-translated from one natural language to another. It works by comparing the machine-generated text to one or more reference texts (typically human-generated) and computes a score indicating how similar they are, based on the presence of the same words and phrases. BLEU is particularly popular in tasks like machine translation but is also used in other contexts like text summarization.\n",
        "\n",
        "2. `rouge_scorer = evaluate.load('rouge')`: This line loads the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scorer. ROUGE is another popular evaluation metric used primarily in summarization tasks. Unlike BLEU, which is precision-oriented, ROUGE focuses on recall, meaning it measures how well the generated summaries cover the content present in the reference summaries. It compares the overlap of n-grams, word sequences, and word pairs between the computer-generated output and the reference texts.\n",
        "\n",
        "These metrics will be used later in the training process to evaluate how well the fine-tuned language model performs on specific NLP tasks, such as translation or summarization. Using these evaluation metrics allows us to quantitatively assess the quality of the generated text and make informed decisions about the model's performance and potential improvements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:12.257274Z",
          "iopub.status.busy": "2024-03-10T12:06:12.256689Z",
          "iopub.status.idle": "2024-03-10T12:06:15.142744Z",
          "shell.execute_reply": "2024-03-10T12:06:15.141698Z",
          "shell.execute_reply.started": "2024-03-10T12:06:12.257246Z"
        },
        "id": "DzY1tS5WAEaB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bleu_scorer = evaluate.load('bleu')\n",
        "rouge_scorer = evaluate.load('rouge')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSyMk-pHBFYy"
      },
      "source": [
        "# Adding some necessary code for OPT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:15.146782Z",
          "iopub.status.busy": "2024-03-10T12:06:15.146480Z",
          "iopub.status.idle": "2024-03-10T12:06:15.281553Z",
          "shell.execute_reply": "2024-03-10T12:06:15.280609Z",
          "shell.execute_reply.started": "2024-03-10T12:06:15.146754Z"
        },
        "id": "TdLgdCKYBE47",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# coding=utf-8\n",
        "# Copyright 2022 The Fairseq Authors and The HuggingFace Inc. team. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" PyTorch OPT model.\"\"\"\n",
        "import random\n",
        "from typing import List, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "\n",
        "from transformers.activations import ACT2FN\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutputWithPast,\n",
        "    CausalLMOutputWithPast,\n",
        "    QuestionAnsweringModelOutput,\n",
        "    SequenceClassifierOutputWithPast,\n",
        ")\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "from transformers.utils import (\n",
        "    add_code_sample_docstrings,\n",
        "    add_start_docstrings,\n",
        "    add_start_docstrings_to_model_forward,\n",
        "    logging,\n",
        "    replace_return_docstrings,\n",
        ")\n",
        "from transformers.models.opt.configuration_opt import OPTConfig\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "_CHECKPOINT_FOR_DOC = \"facebook/opt-350m\"\n",
        "_CONFIG_FOR_DOC = \"OPTConfig\"\n",
        "\n",
        "# Base model docstring\n",
        "_EXPECTED_OUTPUT_SHAPE = [1, 8, 1024]\n",
        "\n",
        "# SequenceClassification docstring\n",
        "_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION = \"ArthurZ/opt-350m-dummy-sc\"\n",
        "_SEQ_CLASS_EXPECTED_LOSS = 1.71\n",
        "_SEQ_CLASS_EXPECTED_OUTPUT = \"'LABEL_0'\"\n",
        "\n",
        "OPT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"facebook/opt-125m\",\n",
        "    \"facebook/opt-350m\",\n",
        "    \"facebook/opt-1.3b\",\n",
        "    \"facebook/opt-2.7b\",\n",
        "    \"facebook/opt-6.7b\",\n",
        "    \"facebook/opt-13b\",\n",
        "    \"facebook/opt-30b\",\n",
        "    # See all OPT models at https://huggingface.co/models?filter=opt\n",
        "]\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bart.modeling_bart._make_causal_mask\n",
        "def _make_causal_mask(\n",
        "        input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0\n",
        "):\n",
        "    \"\"\"\n",
        "    Make causal mask used for bi-directional self-attention.\n",
        "    \"\"\"\n",
        "    bsz, tgt_len = input_ids_shape\n",
        "    mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min, device=device), device=device)\n",
        "    mask_cond = torch.arange(mask.size(-1), device=device)\n",
        "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
        "    mask = mask.to(dtype)\n",
        "\n",
        "    if past_key_values_length > 0:\n",
        "        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n",
        "    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
        "\n",
        "\n",
        "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
        "    \"\"\"\n",
        "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
        "    \"\"\"\n",
        "    bsz, src_len = mask.size()\n",
        "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
        "\n",
        "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
        "\n",
        "    inverted_mask = 1.0 - expanded_mask\n",
        "\n",
        "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
        "\n",
        "\n",
        "class OPTLearnedPositionalEmbedding(nn.Embedding):\n",
        "    \"\"\"\n",
        "    This module learns positional embeddings up to a fixed maximum size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
        "        # OPT is set up so that if padding_idx is specified then offset the embedding ids by 2\n",
        "        # and adjust num_embeddings appropriately. Other models don't have this hack\n",
        "        self.offset = 2\n",
        "        super().__init__(num_embeddings + self.offset, embedding_dim)\n",
        "\n",
        "    def forward(self, attention_mask: torch.LongTensor, past_key_values_length: int = 0):\n",
        "        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n",
        "        attention_mask = attention_mask.long()\n",
        "\n",
        "        # create positions depending on attention_mask\n",
        "        positions = (torch.cumsum(attention_mask, dim=1).type_as(attention_mask) * attention_mask).long() - 1\n",
        "\n",
        "        # cut positions if `past_key_values_length` is > 0\n",
        "        positions = positions[:, past_key_values_length:]\n",
        "\n",
        "        return super().forward(positions + self.offset)\n",
        "\n",
        "\n",
        "class OPTDecoderLayer(nn.Module):\n",
        "    def __init__(self, config: OPTConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.hidden_size\n",
        "        self.self_attn = OPTAttention(\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_heads=config.num_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            is_decoder=True,\n",
        "            bias=config.enable_bias,\n",
        "        )\n",
        "        self.do_layer_norm_before = config.do_layer_norm_before\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(\n",
        "            self.embed_dim, elementwise_affine=config.layer_norm_elementwise_affine\n",
        "        )\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.ffn_dim, bias=config.enable_bias)\n",
        "        self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim, bias=config.enable_bias)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.embed_dim, elementwise_affine=config.layer_norm_elementwise_affine)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            hidden_states: torch.Tensor,\n",
        "            attention_mask: Optional[torch.Tensor] = None,\n",
        "            layer_head_mask: Optional[torch.Tensor] = None,\n",
        "            past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "            output_attentions: Optional[bool] = False,\n",
        "            use_cache: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
        "            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n",
        "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
        "            layer_head_mask (`torch.FloatTensor`, *optional*): mask for attention heads in a given layer of size\n",
        "                `(encoder_attention_heads,)`.\n",
        "            output_attentions (`bool`, *optional*):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "                returned tensors for more detail.\n",
        "            use_cache (`bool`, *optional*):\n",
        "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
        "                (see `past_key_values`).\n",
        "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
        "        \"\"\"\n",
        "\n",
        "        residual = hidden_states\n",
        "\n",
        "        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\n",
        "        if self.do_layer_norm_before:\n",
        "            hidden_states = self.self_attn_layer_norm(hidden_states)\n",
        "\n",
        "        # Self Attention\n",
        "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            past_key_value=past_key_value,\n",
        "            attention_mask=attention_mask,\n",
        "            layer_head_mask=layer_head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "\n",
        "        # 350m applies layer norm AFTER attention\n",
        "        if not self.do_layer_norm_before:\n",
        "            hidden_states = self.self_attn_layer_norm(hidden_states)\n",
        "\n",
        "        # Fully Connected\n",
        "        hidden_states_shape = hidden_states.shape\n",
        "        hidden_states = hidden_states.reshape(-1, hidden_states.size(-1))\n",
        "        residual = hidden_states\n",
        "\n",
        "        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\n",
        "        if self.do_layer_norm_before:\n",
        "            hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "        hidden_states = self.fc1(hidden_states)\n",
        "        hidden_states = self.activation_fn(hidden_states)\n",
        "\n",
        "        hidden_states = self.fc2(hidden_states)\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "\n",
        "        hidden_states = (residual + hidden_states).view(hidden_states_shape)\n",
        "\n",
        "        # 350m applies layer norm AFTER attention\n",
        "        if not self.do_layer_norm_before:\n",
        "            hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (self_attn_weights,)\n",
        "\n",
        "        if use_cache:\n",
        "            outputs += (present_key_value,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "OPT_START_DOCSTRING = r\"\"\"\n",
        "    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
        "    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
        "    etc.)\n",
        "\n",
        "    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
        "    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
        "    and behavior.\n",
        "\n",
        "    Parameters:\n",
        "        config ([`OPTConfig`]):\n",
        "            Model configuration class with all the parameters of the model. Initializing with a config file does not\n",
        "            load the weights associated with the model, only the configuration. Check out the\n",
        "            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare OPT Model outputting raw hidden-states without any specific head on top.\",\n",
        "    OPT_START_DOCSTRING,\n",
        ")\n",
        "class OPTPreTrainedModel(PreTrainedModel):\n",
        "    config_class = OPTConfig\n",
        "    base_model_prefix = \"model\"\n",
        "    supports_gradient_checkpointing = True\n",
        "    _no_split_modules = [\"OPTDecoderLayer\"]\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"decoder\\.version\"]\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = self.config.init_std\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "\n",
        "    def _set_gradient_checkpointing(self, module, value=False):\n",
        "        if isinstance(module, (OPTDecoder)):\n",
        "            module.gradient_checkpointing = value\n",
        "\n",
        "\n",
        "OPT_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
        "            it.\n",
        "\n",
        "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "            [`PreTrainedTokenizer.__call__`] for details.\n",
        "\n",
        "            [What are input IDs?](../glossary#input-ids)\n",
        "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "\n",
        "            [What are attention masks?](../glossary#attention-mask)\n",
        "\n",
        "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "            [`PreTrainedTokenizer.__call__`] for details.\n",
        "\n",
        "            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n",
        "            `past_key_values`).\n",
        "\n",
        "            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n",
        "            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n",
        "            information on the default strategy.\n",
        "        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
        "            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n",
        "\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "\n",
        "        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
        "            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
        "            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
        "            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
        "\n",
        "            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
        "            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
        "\n",
        "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
        "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
        "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
        "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
        "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
        "            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
        "            model's internal embedding lookup matrix.\n",
        "        use_cache (`bool`, *optional*):\n",
        "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
        "            `past_key_values`).\n",
        "        output_attentions (`bool`, *optional*):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (`bool`, *optional*):\n",
        "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (`bool`, *optional*):\n",
        "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class OPTDecoder(OPTPreTrainedModel):\n",
        "    \"\"\"\n",
        "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`OPTDecoderLayer`]\n",
        "\n",
        "    Args:\n",
        "        config: OPTConfig\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: OPTConfig):\n",
        "        super().__init__(config)\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.layerdrop\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.max_target_positions = config.max_position_embeddings\n",
        "        self.vocab_size = config.vocab_size\n",
        "\n",
        "        self.embed_tokens = nn.Embedding(config.vocab_size, config.word_embed_proj_dim, self.padding_idx)\n",
        "        self.embed_positions = OPTLearnedPositionalEmbedding(config.max_position_embeddings, config.hidden_size)\n",
        "\n",
        "        if config.word_embed_proj_dim != config.hidden_size:\n",
        "            self.project_out = nn.Linear(config.hidden_size, config.word_embed_proj_dim, bias=False)\n",
        "        else:\n",
        "            self.project_out = None\n",
        "\n",
        "        if config.word_embed_proj_dim != config.hidden_size:\n",
        "            self.project_in = nn.Linear(config.word_embed_proj_dim, config.hidden_size, bias=False)\n",
        "        else:\n",
        "            self.project_in = None\n",
        "\n",
        "        # Note that the only purpose of `config._remove_final_layer_norm` is to keep backward compatibility\n",
        "        # with checkpoints that have been fine-tuned before transformers v4.20.1\n",
        "        # see https://github.com/facebookresearch/metaseq/pull/164\n",
        "        if config.do_layer_norm_before and not config._remove_final_layer_norm:\n",
        "            self.final_layer_norm = nn.LayerNorm(\n",
        "                config.hidden_size, elementwise_affine=config.layer_norm_elementwise_affine\n",
        "            )\n",
        "        else:\n",
        "            self.final_layer_norm = None\n",
        "\n",
        "        self.layers = nn.ModuleList([OPTDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "        self.gradient_checkpointing = False\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed_tokens = value\n",
        "\n",
        "    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n",
        "    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n",
        "        # create causal mask\n",
        "        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "        combined_attention_mask = None\n",
        "        if input_shape[-1] > 1:\n",
        "            combined_attention_mask = _make_causal_mask(\n",
        "                input_shape,\n",
        "                inputs_embeds.dtype,\n",
        "                device=inputs_embeds.device,\n",
        "                past_key_values_length=past_key_values_length,\n",
        "            )\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n",
        "                inputs_embeds.device\n",
        "            )\n",
        "            combined_attention_mask = (\n",
        "                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n",
        "            )\n",
        "\n",
        "        return combined_attention_mask\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids: torch.LongTensor = None,\n",
        "            attention_mask: Optional[torch.Tensor] = None,\n",
        "            head_mask: Optional[torch.Tensor] = None,\n",
        "            past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "            inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "            use_cache: Optional[bool] = None,\n",
        "            output_attentions: Optional[bool] = None,\n",
        "            output_hidden_states: Optional[bool] = None,\n",
        "            return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
        "                provide it.\n",
        "\n",
        "                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "                [`PreTrainedTokenizer.__call__`] for details.\n",
        "\n",
        "                [What are input IDs?](../glossary#input-ids)\n",
        "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "\n",
        "                [What are attention masks?](../glossary#attention-mask)\n",
        "            head_mask (`torch.Tensor` of shape `(num_hidden_layers, num_attention_heads)`, *optional*):\n",
        "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
        "\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the head is **masked**.\n",
        "\n",
        "            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
        "                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n",
        "                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
        "\n",
        "                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n",
        "                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
        "\n",
        "                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n",
        "                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n",
        "                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
        "\n",
        "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
        "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
        "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
        "                than the model's internal embedding lookup matrix.\n",
        "            output_attentions (`bool`, *optional*):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "                returned tensors for more detail.\n",
        "            output_hidden_states (`bool`, *optional*):\n",
        "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
        "                for more detail.\n",
        "            return_dict (`bool`, *optional*):\n",
        "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # retrieve input_ids and inputs_embeds\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input_ids)\n",
        "\n",
        "        batch_size, seq_length = input_shape\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "        # required mask seq length can be calculated via length of past\n",
        "        mask_seq_length = past_key_values_length + seq_length\n",
        "\n",
        "        # embed positions\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n",
        "        elif attention_mask.shape[1] != mask_seq_length:\n",
        "            raise ValueError(\n",
        "                f\"The provided attention mask has length {attention_mask.shape[1]}, but its length should be \"\n",
        "                f\"{mask_seq_length} (sum of the lengths of current and past inputs)\"\n",
        "            )\n",
        "        causal_attention_mask = self._prepare_decoder_attention_mask(\n",
        "            attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
        "        )\n",
        "        pos_embeds = self.embed_positions(attention_mask, past_key_values_length)\n",
        "\n",
        "        if self.project_in is not None:\n",
        "            inputs_embeds = self.project_in(inputs_embeds)\n",
        "\n",
        "        hidden_states = inputs_embeds + pos_embeds\n",
        "\n",
        "        if self.gradient_checkpointing and self.training:\n",
        "            if use_cache:\n",
        "                logger.warning_once(\n",
        "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
        "                )\n",
        "                use_cache = False\n",
        "\n",
        "        # decoder layers\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attns = () if output_attentions else None\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "\n",
        "        # check if head_mask has a correct number of layers specified if desired\n",
        "        for attn_mask, mask_name in zip([head_mask], [\"head_mask\"]):\n",
        "            if attn_mask is not None:\n",
        "                if attn_mask.size()[0] != (len(self.layers)):\n",
        "                    raise ValueError(\n",
        "                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n",
        "                        f\" {head_mask.size()[0]}.\"\n",
        "                    )\n",
        "\n",
        "        for idx, decoder_layer in enumerate(self.layers):\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states += (hidden_states,)\n",
        "\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):\n",
        "                continue\n",
        "\n",
        "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
        "\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        # None for past_key_value\n",
        "                        return module(*inputs, output_attentions, None)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(decoder_layer),\n",
        "                    hidden_states,\n",
        "                    causal_attention_mask,\n",
        "                    head_mask[idx] if head_mask is not None else None,\n",
        "                    None,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = decoder_layer(\n",
        "                    hidden_states,\n",
        "                    attention_mask=causal_attention_mask,\n",
        "                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
        "                    past_key_value=past_key_value,\n",
        "                    output_attentions=output_attentions,\n",
        "                    use_cache=use_cache,\n",
        "                )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n",
        "\n",
        "            if output_attentions:\n",
        "                all_self_attns += (layer_outputs[1],)\n",
        "\n",
        "        if self.final_layer_norm is not None:\n",
        "            hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "        if self.project_out is not None:\n",
        "            hidden_states = self.project_out(hidden_states)\n",
        "\n",
        "        # add hidden states from the last decoder layer\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states += (hidden_states,)\n",
        "\n",
        "        next_cache = next_decoder_cache if use_cache else None\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
        "        return BaseModelOutputWithPast(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attns,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare OPT Model outputting raw hidden-states without any specific head on top.\",\n",
        "    OPT_START_DOCSTRING,\n",
        ")\n",
        "class OPTModel(OPTPreTrainedModel):\n",
        "    def __init__(self, config: OPTConfig):\n",
        "        super().__init__(config)\n",
        "        self.decoder = OPTDecoder(config)\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.decoder.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.decoder.embed_tokens = value\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.decoder\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(OPT_INPUTS_DOCSTRING)\n",
        "    @add_code_sample_docstrings(\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=BaseModelOutputWithPast,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "        expected_output=_EXPECTED_OUTPUT_SHAPE,\n",
        "    )\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids: torch.LongTensor = None,\n",
        "            attention_mask: Optional[torch.Tensor] = None,\n",
        "            head_mask: Optional[torch.Tensor] = None,\n",
        "            past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "            inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "            use_cache: Optional[bool] = None,\n",
        "            output_attentions: Optional[bool] = None,\n",
        "            output_hidden_states: Optional[bool] = None,\n",
        "            return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n",
        "        decoder_outputs = self.decoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        if not return_dict:\n",
        "            return decoder_outputs\n",
        "\n",
        "        return BaseModelOutputWithPast(\n",
        "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
        "            past_key_values=decoder_outputs.past_key_values,\n",
        "            hidden_states=decoder_outputs.hidden_states,\n",
        "            attentions=decoder_outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class OPTForCausalLM(OPTPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.model = OPTModel(config)\n",
        "\n",
        "        # the lm_head weight is automatically tied to the embed tokens weight\n",
        "        self.lm_head = nn.Linear(config.word_embed_proj_dim, config.vocab_size, bias=False)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.model.decoder.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.model.decoder.embed_tokens = value\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    def set_decoder(self, decoder):\n",
        "        self.model.decoder = decoder\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.model.decoder\n",
        "\n",
        "    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids: torch.LongTensor = None,\n",
        "            attention_mask: Optional[torch.Tensor] = None,\n",
        "            head_mask: Optional[torch.Tensor] = None,\n",
        "            past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "            inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "            labels: Optional[torch.LongTensor] = None,\n",
        "            use_cache: Optional[bool] = None,\n",
        "            output_attentions: Optional[bool] = None,\n",
        "            output_hidden_states: Optional[bool] = None,\n",
        "            return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
        "                provide it.\n",
        "\n",
        "                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "                [`PreTrainedTokenizer.__call__`] for details.\n",
        "\n",
        "                [What are input IDs?](../glossary#input-ids)\n",
        "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "\n",
        "                [What are attention masks?](../glossary#attention-mask)\n",
        "            head_mask (`torch.Tensor` of shape `(num_hidden_layers, num_attention_heads)`, *optional*):\n",
        "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
        "\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the head is **masked**.\n",
        "\n",
        "            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
        "                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n",
        "                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
        "                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional\n",
        "                tensors are only required when the model is used as a decoder in a Sequence to Sequence model.\n",
        "\n",
        "                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n",
        "                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
        "\n",
        "                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n",
        "                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n",
        "                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
        "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
        "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
        "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
        "                than the model's internal embedding lookup matrix.\n",
        "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
        "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
        "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
        "            use_cache (`bool`, *optional*):\n",
        "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
        "                (see `past_key_values`).\n",
        "            output_attentions (`bool`, *optional*):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "                returned tensors for more detail.\n",
        "            output_hidden_states (`bool`, *optional*):\n",
        "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
        "                for more detail.\n",
        "            return_dict (`bool`, *optional*):\n",
        "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        Example:\n",
        "\n",
        "        ```python\n",
        "        >>> from transformers import AutoTokenizer, OPTForCausalLM\n",
        "\n",
        "        >>> model = OPTForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
        "        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "\n",
        "        >>> prompt = \"Hey, are you consciours? Can you talk to me?\"\n",
        "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "        >>> # Generate\n",
        "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
        "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "        \"Hey, are you consciours? Can you talk to me?\\nI'm not consciours, but I can talk to you.\"\n",
        "        ```\"\"\"\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
        "        outputs = self.model.decoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        logits = self.lm_head(outputs[0]).contiguous()\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # move labels to correct device to enable model parallelism\n",
        "            labels = labels.to(logits.device)\n",
        "            # Shift so that tokens < n predict n\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            # Flatten the tokens\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[1:]\n",
        "            return (loss,) + output if loss is not None else output\n",
        "\n",
        "        return CausalLMOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(\n",
        "            self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n",
        "    ):\n",
        "        if past_key_values:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "\n",
        "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
        "        if inputs_embeds is not None and past_key_values is None:\n",
        "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
        "        else:\n",
        "            model_inputs = {\"input_ids\": input_ids}\n",
        "\n",
        "        model_inputs.update(\n",
        "            {\n",
        "                \"past_key_values\": past_key_values,\n",
        "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
        "                \"attention_mask\": attention_mask,\n",
        "            }\n",
        "        )\n",
        "        return model_inputs\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past_key_values, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past_key_values:\n",
        "            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n",
        "        return reordered_past\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    The OPT Model transformer with a sequence classification head on top (linear layer).\n",
        "\n",
        "    [`OPTForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n",
        "    (e.g. GPT-2) do.\n",
        "\n",
        "    Since it does classification on the last token, it requires to know the position of the last token. If a\n",
        "    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n",
        "    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n",
        "    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n",
        "    each row of the batch).\n",
        "    \"\"\",\n",
        "    OPT_START_DOCSTRING,\n",
        ")\n",
        "class OPTForSequenceClassification(OPTPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n",
        "\n",
        "    def __init__(self, config: OPTConfig):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.model = OPTModel(config)\n",
        "        self.score = nn.Linear(config.word_embed_proj_dim, self.num_labels, bias=False)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(OPT_INPUTS_DOCSTRING)\n",
        "    @add_code_sample_docstrings(\n",
        "        checkpoint=_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION,\n",
        "        output_type=SequenceClassifierOutputWithPast,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "        expected_output=_SEQ_CLASS_EXPECTED_OUTPUT,\n",
        "        expected_loss=_SEQ_CLASS_EXPECTED_LOSS,\n",
        "    )\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids: Optional[torch.LongTensor] = None,\n",
        "            attention_mask: Optional[torch.FloatTensor] = None,\n",
        "            head_mask: Optional[torch.FloatTensor] = None,\n",
        "            past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
        "            inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "            labels: Optional[torch.LongTensor] = None,\n",
        "            use_cache: Optional[bool] = None,\n",
        "            output_attentions: Optional[bool] = None,\n",
        "            output_hidden_states: Optional[bool] = None,\n",
        "            return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
        "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
        "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        transformer_outputs = self.model(\n",
        "            input_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        hidden_states = transformer_outputs[0]\n",
        "        logits = self.score(hidden_states)\n",
        "\n",
        "        if input_ids is not None:\n",
        "            batch_size, sequence_length = input_ids.shape[:2]\n",
        "        else:\n",
        "            batch_size, sequence_length = inputs_embeds.shape[:2]\n",
        "\n",
        "        if self.config.pad_token_id is None:\n",
        "            sequence_lengths = -1\n",
        "        else:\n",
        "            if input_ids is not None:\n",
        "                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\n",
        "            else:\n",
        "                sequence_lengths = -1\n",
        "                logger.warning(\n",
        "                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n",
        "                    \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n",
        "                )\n",
        "\n",
        "        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(pooled_logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(pooled_logits, labels)\n",
        "        if not return_dict:\n",
        "            output = (pooled_logits,) + transformer_outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=pooled_logits,\n",
        "            past_key_values=transformer_outputs.past_key_values,\n",
        "            hidden_states=transformer_outputs.hidden_states,\n",
        "            attentions=transformer_outputs.attentions,\n",
        "        )\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.model.decoder.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.model.decoder.embed_tokens = value\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    The OPT Model transformer with a span classification head on top for extractive question-answering tasks like SQuAD\n",
        "    (a linear layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n",
        "    \"\"\",\n",
        "    OPT_START_DOCSTRING,\n",
        ")\n",
        "class OPTForQuestionAnswering(OPTPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n",
        "\n",
        "    def __init__(self, config: OPTConfig):\n",
        "        super().__init__(config)\n",
        "        self.model = OPTModel(config)\n",
        "        self.qa_outputs = nn.Linear(config.word_embed_proj_dim, 2)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(OPT_INPUTS_DOCSTRING)\n",
        "    @replace_return_docstrings(output_type=QuestionAnsweringModelOutput, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids: Optional[torch.LongTensor] = None,\n",
        "            attention_mask: Optional[torch.FloatTensor] = None,\n",
        "            head_mask: Optional[torch.FloatTensor] = None,\n",
        "            past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
        "            inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "            start_positions: Optional[torch.LongTensor] = None,\n",
        "            end_positions: Optional[torch.LongTensor] = None,\n",
        "            use_cache: Optional[bool] = None,\n",
        "            output_attentions: Optional[bool] = None,\n",
        "            output_hidden_states: Optional[bool] = None,\n",
        "            return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n",
        "        r\"\"\"\n",
        "        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n",
        "            are not taken into account for computing the loss.\n",
        "        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n",
        "            are not taken into account for computing the loss.\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        Example:\n",
        "\n",
        "        ```python\n",
        "        >>> from transformers import AutoTokenizer, OPTForQuestionAnswering\n",
        "        >>> import torch\n",
        "\n",
        "        >>> torch.manual_seed(4)  # doctest: +IGNORE_RESULT\n",
        "        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "\n",
        "        >>> # note: we are loading a OPTForQuestionAnswering from the hub here,\n",
        "        >>> # so the head will be randomly initialized, hence the predictions will be random\n",
        "        >>> model = OPTForQuestionAnswering.from_pretrained(\"facebook/opt-350m\")\n",
        "\n",
        "        >>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
        "\n",
        "        >>> inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
        "        >>> with torch.no_grad():\n",
        "        ...     outputs = model(**inputs)\n",
        "\n",
        "        >>> answer_start_index = outputs.start_logits.argmax()\n",
        "        >>> answer_end_index = outputs.end_logits.argmax()\n",
        "\n",
        "        >>> answer_offset = len(tokenizer(question)[0])\n",
        "\n",
        "        >>> predict_answer_tokens = inputs.input_ids[\n",
        "        ...     0, answer_offset + answer_start_index : answer_offset + answer_end_index + 1\n",
        "        ... ]\n",
        "        >>> predicted = tokenizer.decode(predict_answer_tokens)\n",
        "        >>> predicted\n",
        "        ' a nice puppet'\n",
        "        ```\"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        transformer_outputs = self.model(\n",
        "            input_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        hidden_states = transformer_outputs[0]\n",
        "\n",
        "        logits = self.qa_outputs(hidden_states)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1).contiguous()\n",
        "        end_logits = end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "        total_loss = None\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions = start_positions.clamp(0, ignored_index)\n",
        "            end_positions = end_positions.clamp(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (start_logits, end_logits) + transformer_outputs[2:]\n",
        "            return ((total_loss,) + output) if total_loss is not None else output\n",
        "\n",
        "        return QuestionAnsweringModelOutput(\n",
        "            loss=total_loss,\n",
        "            start_logits=start_logits,\n",
        "            end_logits=end_logits,\n",
        "            hidden_states=transformer_outputs.hidden_states,\n",
        "            attentions=transformer_outputs.attentions,\n",
        "        )\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.model.decoder.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.model.decoder.embed_tokens = value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1NLMeSQA00T"
      },
      "source": [
        "# Task1: Implement LoRA in the OPTAttention Class\n",
        "\n",
        "In this task, you will enhance the `OPTAttention` class by integrating Low-Rank Adaptation (LoRA) into the multi-headed attention mechanism. This modification aims to improve the model's adaptability and efficiency, making it more suitable for fine-tuning purposes.\n",
        "\n",
        "#### Requirements:\n",
        "1. Understand the structure and functionality of the multi-headed attention mechanism as introduced in the \"Attention Is All You Need\" paper.\n",
        "2. Gain familiarity with the concept of Low-Rank Adaptation (LoRA) and how it can be applied to neural network layers, specifically within the context of attention mechanisms.\n",
        "\n",
        "#### Instructions:\n",
        "\n",
        "1. **LoRA Parameters Initialization**:\n",
        "    - In the `__init__` method of the `OPTAttention` class, initialize the LoRA parameters `lora_A_k`, `lora_B_k`, `lora_A_v`, and `lora_B_v` following the standard attention parameters initialization.\n",
        "    - These parameters introduce low-rank matrices that will modify the standard key and value projections within the attention mechanism. The matrices `lora_A_k` and `lora_A_v` reduce the dimension from the embedding dimension to a smaller rank, while `lora_B_k` and `lora_B_v` project them back to the original space.\n",
        "\n",
        "2. **Implementing LoRA in Attention Projections**:\n",
        "    - Within the `forward` method, integrate the LoRA parameters with the attention mechanism. Specifically, modify the `key_states` and `value_states` calculations where indicated by the comments.\n",
        "    - Apply LoRA adaptation by combining the original key and value states (obtained from the standard projections) with the outcomes of passing `key_value_states` or `hidden_states` through the LoRA matrices.\n",
        "    - Ensure proper tensor shapes and dimensions when combining LoRA-adapted projections with the standard projections to ensure model consistency and correctness.\n",
        "\n",
        "3. **Scaling and Regularization with LoRA**:\n",
        "    - Apply `lora_scaling` to the LoRA-adapted components before adding them to the standard key and value projections. `lora_scaling` controls the impact of LoRA modifications on the model and is computed as `lora_alpha / rank`, where `lora_alpha` is a predefined scaling factor and `rank` is the dimensionality reduction factor.\n",
        "    - Implement `lora_dropout` by applying dropout to the reduced-dimension representations in the LoRA layers. This helps in regularizing the LoRA modifications and preventing overfitting.\n",
        "\n",
        "4. **Validation**:\n",
        "    - Validate your implementation by ensuring it passes any provided tests or validation checks. Pay particular attention to the tensor shapes and the logical flow of your LoRA modifications to ensure they align with the expected functionality of the multi-headed attention mechanism.\n",
        "\n",
        "#### Example Implementation:\n",
        "Below is an example of how you might modify a section of the code for LoRA integration within the attention projections:\n",
        "\n",
        "```python\n",
        "# Original line for context:\n",
        "key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
        "\n",
        "# Modified line with LoRA:\n",
        "key_states = self._shape(\n",
        "    # The LoRA Operation, please refer to the paper for more detail\n",
        ")\n",
        "#Please aware of the comment with ###\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:15.283102Z",
          "iopub.status.busy": "2024-03-10T12:06:15.282817Z",
          "iopub.status.idle": "2024-03-10T12:06:15.315264Z",
          "shell.execute_reply": "2024-03-10T12:06:15.314368Z",
          "shell.execute_reply.started": "2024-03-10T12:06:15.283078Z"
        },
        "id": "-tuiWOBKA0XL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class OPTAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embed_dim: int,\n",
        "            num_heads: int,\n",
        "            dropout: float = 0.0,\n",
        "            is_decoder: bool = False,\n",
        "            bias: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        if (self.head_dim * num_heads) != self.embed_dim:\n",
        "            raise ValueError(\n",
        "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
        "                f\" and `num_heads`: {num_heads}).\"\n",
        "            )\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "        self.is_decoder = is_decoder\n",
        "\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "\n",
        "        ### Your code here:\n",
        "        rank = 16\n",
        "        lora_alpha = 32\n",
        "        lora_dropout = 0.05\n",
        "        self.lora_A_k = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.lora_B_k = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.lora_A_v = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.lora_B_v = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.lora_scaling = scaling\n",
        "        self.lora_dropout = self.lora_dropout\n",
        "        ### End of code writing \n",
        "\n",
        "\n",
        "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
        "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            hidden_states: torch.Tensor,\n",
        "            key_value_states: Optional[torch.Tensor] = None,\n",
        "            past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "            attention_mask: Optional[torch.Tensor] = None,\n",
        "            layer_head_mask: Optional[torch.Tensor] = None,\n",
        "            output_attentions: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
        "\n",
        "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
        "        # for the decoder\n",
        "        is_cross_attention = key_value_states is not None\n",
        "\n",
        "        bsz, tgt_len, _ = hidden_states.size()\n",
        "\n",
        "        # get query proj\n",
        "        query_states = self.q_proj(hidden_states) * self.scaling\n",
        "        # get key, value proj\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_states = past_key_value[0]\n",
        "            value_states = past_key_value[1]\n",
        "        elif is_cross_attention:\n",
        "            # cross_attentions\n",
        "            ### Your code here: add lora to key value state\n",
        "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
        "        elif past_key_value is not None:\n",
        "            # reuse k, v, self_attention\n",
        "            ### Your code here: add lora to key value state\n",
        "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "        else:\n",
        "            # self_attention\n",
        "            ### Your code here: add lora to key value state\n",
        "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_states, value_states)\n",
        "\n",
        "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
        "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
        "        key_states = key_states.view(*proj_shape)\n",
        "        value_states = value_states.view(*proj_shape)\n",
        "\n",
        "        src_len = key_states.size(1)\n",
        "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
        "\n",
        "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
        "            raise ValueError(\n",
        "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
        "                f\" {attn_weights.size()}\"\n",
        "            )\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
        "                raise ValueError(\n",
        "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
        "                )\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
        "            attn_weights = torch.max(\n",
        "                attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min, device=attn_weights.device)\n",
        "            )\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        # upcast to fp32 if the weights are in fp16. Please see https://github.com/huggingface/transformers/pull/17437\n",
        "        if attn_weights.dtype == torch.float16:\n",
        "            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(torch.float16)\n",
        "        else:\n",
        "            attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
        "\n",
        "        if layer_head_mask is not None:\n",
        "            if layer_head_mask.size() != (self.num_heads,):\n",
        "                raise ValueError(\n",
        "                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n",
        "                    f\" {layer_head_mask.size()}\"\n",
        "                )\n",
        "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if output_attentions:\n",
        "            # this operation is a bit awkward, but it's required to\n",
        "            # make sure that attn_weights keeps its gradient.\n",
        "            # In order to do so, attn_weights have to be reshaped\n",
        "            # twice and have to be reused in the following\n",
        "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "        else:\n",
        "            attn_weights_reshaped = None\n",
        "\n",
        "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "\n",
        "        attn_output = torch.bmm(attn_probs, value_states)\n",
        "\n",
        "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
        "            raise ValueError(\n",
        "                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
        "                f\" {attn_output.size()}\"\n",
        "            )\n",
        "\n",
        "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
        "        attn_output = attn_output.transpose(1, 2)\n",
        "\n",
        "        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n",
        "        # partitioned aross GPUs when using tensor-parallelism.\n",
        "        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
        "\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "\n",
        "        return attn_output, attn_weights_reshaped, past_key_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0lY-JaeCOvM"
      },
      "source": [
        "# Initialize the model\n",
        "- In this tutorial, we use OPT-2.7B as the base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:15.316882Z",
          "iopub.status.busy": "2024-03-10T12:06:15.316554Z",
          "iopub.status.idle": "2024-03-10T12:06:38.223423Z",
          "shell.execute_reply": "2024-03-10T12:06:38.222403Z",
          "shell.execute_reply.started": "2024-03-10T12:06:15.316858Z"
        },
        "id": "gHu_vmH9CF_H",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacebook/opt-2.7b\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mOPTForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/VScode/DeepLearning/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:3193\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3191\u001b[0m         \u001b[38;5;66;03m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[1;32m   3192\u001b[0m         filename \u001b[38;5;241m=\u001b[39m _add_variant(WEIGHTS_NAME, variant)\n\u001b[0;32m-> 3193\u001b[0m         resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3194\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\n\u001b[1;32m   3195\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(WEIGHTS_NAME, variant):\n\u001b[1;32m   3197\u001b[0m     \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[1;32m   3198\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m   3199\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3200\u001b[0m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[1;32m   3201\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs,\n\u001b[1;32m   3202\u001b[0m     )\n",
            "File \u001b[0;32m~/VScode/DeepLearning/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    413\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
            "File \u001b[0;32m~/VScode/DeepLearning/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/VScode/DeepLearning/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1492\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1490\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[0;32m-> 1492\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1503\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/VScode/DeepLearning/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:547\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries)\u001b[0m\n\u001b[1;32m    545\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 547\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[1;32m    549\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/VScode/DeepLearning/.venv/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
            "File \u001b[0;32m~/VScode/DeepLearning/.venv/lib/python3.11/site-packages/urllib3/response.py:1043\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1043\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1046\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
            "File \u001b[0;32m~/VScode/DeepLearning/.venv/lib/python3.11/site-packages/urllib3/response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 935\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m~/VScode/DeepLearning/.venv/lib/python3.11/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 862\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/VScode/DeepLearning/.venv/lib/python3.11/site-packages/urllib3/response.py:845\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
            "File \u001b[0;32m/usr/lib/python3.11/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
            "File \u001b[0;32m/usr/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.11/ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m/usr/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model_name = 'facebook/opt-2.7b'\n",
        "model = OPTForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zes4OQZfCsz2"
      },
      "source": [
        "# Task2: Post-Initialization and Freezing Model Weights\n",
        "\n",
        "In this section, you will perform two critical steps necessary for preparing your model for LoRA-based fine-tuning: post-initialization of LoRA parameters and freezing the base model weights. Follow the instructions below to understand and implement these steps in your code.\n",
        "\n",
        "#### Post-Initialization of LoRA Parameters:\n",
        "After integrating LoRA into your model, it is crucial to initialize the parameters correctly:\n",
        "\n",
        "1. **Kaiming Uniform Initialization for LoRA_A**:\n",
        "   - Use the `nn.init.kaiming_uniform_` function to initialize parameters associated with `lora_A`.\n",
        "   - This type of initialization is well-suited for layers followed by ReLU activations, as it considers the size of the previous layer to maintain a consistent variance of activations.\n",
        "   - The parameter `a` is set based on the rectifier linear unit's negative slope, optimizing the initialization for layers that use ReLU or similar activations.\n",
        "\n",
        "2. **Zero Initialization for LoRA_B**:\n",
        "   - Initialize the `lora_B` parameters with zeros using `nn.init.zeros_`.\n",
        "   - This ensures that the LoRA modifications start from a neutral state, allowing the adapted parts of the model to learn from scratch during training.\n",
        "\n",
        "#### Freezing Base Model Weights:\n",
        "To focus the learning process on the LoRA parameters while preserving the pre-trained knowledge in other parts of the model, you need to freeze the base model weights:\n",
        "\n",
        "1. **Set `requires_grad` to False for Non-LoRA Parameters**:\n",
        "   - Iterate through all named parameters in the model. If a parameter's name does not contain 'lora', set its `requires_grad` attribute to `False`.\n",
        "   - This action prevents the standard backpropagation updates from modifying these parameters, effectively freezing them during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:38.225236Z",
          "iopub.status.busy": "2024-03-10T12:06:38.224916Z",
          "iopub.status.idle": "2024-03-10T12:06:38.268161Z",
          "shell.execute_reply": "2024-03-10T12:06:38.267370Z",
          "shell.execute_reply.started": "2024-03-10T12:06:38.225210Z"
        },
        "id": "UnVhkxiPAU-D",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Write your code following the instructions\n",
        "# do post initialization on lora A and lora B\n",
        "\n",
        "\n",
        "# freeze base model weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:38.269625Z",
          "iopub.status.busy": "2024-03-10T12:06:38.269267Z",
          "iopub.status.idle": "2024-03-10T12:06:40.317132Z",
          "shell.execute_reply": "2024-03-10T12:06:40.316144Z",
          "shell.execute_reply.started": "2024-03-10T12:06:38.269594Z"
        },
        "id": "vFkvrMCaC7r-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# enable gradient checkpoint for lower gpu memory usage\n",
        "model.gradient_checkpointing_enable()\n",
        "# enable input gradient for lora training\n",
        "model.enable_input_require_grads()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIGbE7feDMS1"
      },
      "source": [
        "# Monitoring Model Parameters\n",
        "\n",
        "In machine learning and deep learning, understanding the structure and capacity of your model is crucial. One key aspect of this is knowing the number of parameters that are being trained. This information can help in diagnosing model complexity, memory usage, and potential overfitting or underfitting scenarios.\n",
        "\n",
        "#### Function Overview: `print_trainable_parameters`\n",
        "This function is designed to provide a clear overview of the parameters within a given model. Specifically, it calculates and prints the total number of parameters, the number of trainable parameters, and the percentage of parameters that are trainable. Here's a breakdown of its functionalities:\n",
        "\n",
        "1. **Total Parameters Count (`all_param`)**: This represents the total number of parameters in the model, including both trainable and non-trainable (frozen) parameters.\n",
        "\n",
        "2. **Trainable Parameters Count (`trainable_params`)**: This is the subset of the total parameters that will be updated during training. Parameters that are not trainable have been frozen and will retain their values during the training process.\n",
        "\n",
        "3. **Trainable Percentage**: This metric provides insight into how much of the model is being actively trained. A lower percentage might indicate that a large portion of the model is frozen, which could be intentional in scenarios like transfer learning or fine-tuning.\n",
        "\n",
        "The function iterates through all parameters in the model, counting the total and trainable parameters, and then prints out these values along with the percentage of parameters that are trainable. This utility can be particularly useful when you are experimenting with different architectures or when fine-tuning pre-trained models.\n",
        "\n",
        "#### Usage:\n",
        "Simply call `print_trainable_parameters(model)` after defining your model. This will output the parameter statistics, giving you a better understanding of your model's capacity and training scope.\n",
        "\n",
        "Example output:\n",
        "```plaintext\n",
        "trainable params: 1024 || all params: 2048 || trainable%: 50.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:40.320853Z",
          "iopub.status.busy": "2024-03-10T12:06:40.320240Z",
          "iopub.status.idle": "2024-03-10T12:06:40.394815Z",
          "shell.execute_reply": "2024-03-10T12:06:40.393704Z",
          "shell.execute_reply.started": "2024-03-10T12:06:40.320817Z"
        },
        "id": "rOJBHBFMC86P",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "    return {\"trainable\": trainable_params, \"all\": all_param, \"trainable%\": 100 * trainable_params / all_param}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm4SKPuKDRai"
      },
      "source": [
        "# Trainable Parameter Check\n",
        "- Just to check how many parameters are trainable in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:40.397102Z",
          "iopub.status.busy": "2024-03-10T12:06:40.396264Z",
          "iopub.status.idle": "2024-03-10T12:06:40.467367Z",
          "shell.execute_reply": "2024-03-10T12:06:40.466285Z",
          "shell.execute_reply.started": "2024-03-10T12:06:40.397069Z"
        },
        "id": "YVtcseVtDQnS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE34EasAEFzL"
      },
      "source": [
        "# Task3: Implementing Custom Dataset for NLP\n",
        "\n",
        "In this task, you will create a custom dataset class, `HGDataset`, using PyTorch for a natural language processing (NLP) task. This custom dataset will facilitate data handling for training and testing NLP models. Follow the instructions below to complete the missing parts of the `HGDataset` class.\n",
        "\n",
        "#### Objectives:\n",
        "1. Learn to handle NLP datasets using PyTorch's Dataset and DataLoader.\n",
        "2. Implement custom processing and loading methods for NLP data.\n",
        "\n",
        "#### Task Instructions:\n",
        "\n",
        "1. **Initialize the Dataset**:\n",
        "   - In the `__init__` method of the `HGDataset` class, complete the code to extract questions and answers from the provided dataset.\n",
        "   - Assign the questions to `self.input_x` and the answers to `self.target`.\n",
        "   - Ensure that each question corresponds to its respective answer by asserting that `input_x` and `target` are of the same length.\n",
        "   - Store the dataset split (e.g., 'train', 'test') in `self.split` for future reference.\n",
        "\n",
        "2. **Implement the `__getitem__` Method**:\n",
        "   - This method should return a single instance of your data. Implement the `__getitem__` method to return a dictionary containing the input question, the target answer, and the split information for a given index `idx`.\n",
        "   - Ensure the method correctly maps an index to the corresponding question and answer in the dataset.\n",
        "\n",
        "3. **Implement the `__len__` Method**:\n",
        "   - This method should return the total number of items in the dataset. Implement the `__len__` method to return the size of `self.input_x`, which corresponds to the total number of questions (and answers).\n",
        "\n",
        "#### Usage:\n",
        "After completing the `HGDataset` class, you can create instances of the dataset for the training and validation splits:\n",
        "\n",
        "```python\n",
        "dataset_name = 'nq_open'  # This is the name of the dataset we are using\n",
        "\n",
        "# Load the raw dataset\n",
        "dataset = load_dataset(dataset_name)\n",
        "\n",
        "# Create instances of HGDataset for training and validation\n",
        "train_dataset = HGDataset(dataset['train'][:10000], 'train')\n",
        "test_dataset = HGDataset(dataset['validation'][:], 'test')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:40.471474Z",
          "iopub.status.busy": "2024-03-10T12:06:40.471110Z",
          "iopub.status.idle": "2024-03-10T12:06:49.282177Z",
          "shell.execute_reply": "2024-03-10T12:06:49.281177Z",
          "shell.execute_reply.started": "2024-03-10T12:06:40.471438Z"
        },
        "id": "d9fICEm2EBnT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "class HGDataset(torch.utils.data.Dataset):\n",
        "    # longest first for batch finder\n",
        "    def __init__(self, dataset, split):\n",
        "        ### Your code here\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ### Your code here\n",
        "\n",
        "    def __len__(self):\n",
        "        ### Your code here\n",
        "\n",
        "\n",
        "dataset_name = 'nq_open'\n",
        "\n",
        "dataset = load_dataset(dataset_name)\n",
        "\n",
        "# You can adjust the dataset scale with your own preference\n",
        "# The total number for training is 87,9K, validation is 3.61K\n",
        "train_dataset = HGDataset(dataset['train'][:1000], 'train')\n",
        "test_dataset = HGDataset(dataset['validation'][:100], 'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8_q7uaNE7hs"
      },
      "source": [
        "# Task4: Implementing Tokenization and Data Collation\n",
        "\n",
        "In this task, you will set up tokenization parameters and implement a custom data collator function for an NLP model. This is essential for preparing text data properly before feeding it into a neural network for training or inference. Follow the instructions below to configure tokenization and complete the missing parts of the `data_collator_customized` function.\n",
        "\n",
        "#### Objectives:\n",
        "1. Configure tokenizer parameters for handling different aspects of text processing.\n",
        "2. Implement a custom logic for batching and preparing NLP data.\n",
        "\n",
        "\n",
        "#### Task Instructions:\n",
        "\n",
        "1. **Implement Custom Data Collator**:\n",
        "   - In the `data_collator_customized` function, complete the code to transform a list of feature dictionaries into a unified batch for model processing.\n",
        "   - Collect batched features by accumulating each key's values from the feature dictionaries.\n",
        "   - Determine the batch's split type (training or inference) based on the 'split' field from the input features.\n",
        "   - Prepare the text for tokenization by concatenating questions and answers, adding special tokens as necessary. Use different formatting based on whether the batch is for training or inference.\n",
        "   - Tokenize the concatenated text using the appropriate tokenizer (Suggestion: `rtokenizer` for processing concatenated text, `tokenizer` for inference scenarios, but you can choose your own preference nevertheless).\n",
        "   - Prepare the final batch dictionary need have `input_ids`, `attention_mask`, and `labels` keys.\n",
        "\n",
        "#### Usage:\n",
        "After completing the setup and implementation, your custom data collator will be used to prepare data batches in trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:49.284239Z",
          "iopub.status.busy": "2024-03-10T12:06:49.283789Z",
          "iopub.status.idle": "2024-03-10T12:06:51.430077Z",
          "shell.execute_reply": "2024-03-10T12:06:51.429206Z",
          "shell.execute_reply.started": "2024-03-10T12:06:49.284199Z"
        },
        "id": "ssS429twEhVu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "\n",
        "MAX_TOKEN_LENGTH = 128\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.padding_side = 'left'\n",
        "tokenizer.truncation_side = 'left'\n",
        "\n",
        "rtokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "rtokenizer.padding_side = 'right'\n",
        "rtokenizer.truncation_side = 'right'\n",
        "\n",
        "def data_collator_customized(features, return_tensors=\"pt\"):\n",
        "    batch = {}\n",
        "    ### Your code here\n",
        "    ### End of code writing\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mqw_9BrFwOg"
      },
      "source": [
        "# Setting Up the Seq2Seq Trainer\n",
        "\n",
        "The provided code block initializes and configures a `Seq2SeqTrainer` from the Hugging Face's Transformers library. This trainer is designed specifically for sequence-to-sequence models, such as those used in translation, summarization, and other NLP tasks where both the input and output are sequences of tokens.\n",
        "\n",
        "#### Trainer Configuration:\n",
        "- `model`: This is the sequence-to-sequence model that you will be training.\n",
        "- `train_dataset`: The dataset used for training the model.\n",
        "- `args`: A set of training arguments configuring how the model should be trained. These arguments include:\n",
        "    - `per_device_train_batch_size`: The batch size per device during training.\n",
        "    - `per_device_eval_batch_size`: The batch size per device during evaluation.\n",
        "    - `gradient_accumulation_steps`: The number of steps to accumulate gradients before performing a backward/update pass.\n",
        "    - `warmup_steps`: The number of steps used for the warm-up phase.\n",
        "    - `num_train_epochs`: The total number of training epochs.\n",
        "    - `learning_rate`: The initial learning rate for the Adam optimizer.\n",
        "    - `bf16`: Enables training using bfloat16 precision on supported GPUs, which can improve performance and reduce memory usage.\n",
        "    - `logging_steps`: How often to log training information.\n",
        "    - `report_to`: Determines the integration backends where the logs should be reported (here, logging is disabled with 'none').\n",
        "    - `remove_unused_columns`: Indicates whether columns not used by the model forward pass should be removed.\n",
        "    - `output_dir`: The directory where the training outputs should be saved.\n",
        "    - `generation_config`: Specific generation settings for the evaluation phase, such as `max_length` and `num_beams`.\n",
        "    - `predict_with_generate`: Determines whether predictions should be made by generating text during evaluation.\n",
        "\n",
        "- `data_collator`: Custom function to prepare the data batches for training and evaluation.\n",
        "\n",
        "# Training Process:\n",
        "- `trainer.train()`: This method starts the training process based on the configurations provided.\n",
        "\n",
        "The trainer automates many tasks associated with training a sequence-to-sequence model, including data loading, batch creation, model optimization, and logging, allowing you to focus on model architecture and data rather than boilerplate training code.\n",
        "\n",
        "---\n",
        "\n",
        "This setup is typical for training state-of-the-art NLP models and provides a flexible, high-level interface for custom sequence-to-sequence tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:51.431681Z",
          "iopub.status.busy": "2024-03-10T12:06:51.431287Z",
          "iopub.status.idle": "2024-03-10T12:14:29.501356Z",
          "shell.execute_reply": "2024-03-10T12:14:29.500411Z",
          "shell.execute_reply.started": "2024-03-10T12:06:51.431646Z"
        },
        "id": "D-KRUG-BFO2y",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "trainer = transformers.Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    args=transformers.Seq2SeqTrainingArguments(\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=1,\n",
        "        warmup_steps=0,\n",
        "        num_train_epochs=1.0,\n",
        "        learning_rate=0.0001,\n",
        "        bf16=False, # If your GPU supports, make it True\n",
        "        fp16=True, # Since we disable the bf16, we use FP16 instead\n",
        "        logging_steps=1,\n",
        "        report_to=['none'],\n",
        "        remove_unused_columns=False,\n",
        "        output_dir='model_output',\n",
        "        generation_config=transformers.GenerationConfig(\n",
        "            max_length=5,\n",
        "            num_beams=1,\n",
        "        ),\n",
        "        predict_with_generate=True,\n",
        "    ),\n",
        "    data_collator=data_collator_customized\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kryezKcG59i"
      },
      "source": [
        "# Processing Evaluation Results\n",
        "\n",
        "The provided code block is part of an evaluation process, typically after a model has been used to generate predictions on a dataset. This section explains the purpose of each line in the code:\n",
        "\n",
        "1. `logits = eval_result.predictions`:\n",
        "    - This line retrieves the predicted logits from the evaluation results. Logits are the raw output scores from the model before applying any activation function like softmax. In the context of NLP and sequence generation, these logits represent the scores assigned to each token being the next token in the sequence.\n",
        "\n",
        "2. `logits[logits == -100] = tokenizer.eos_token_id`:\n",
        "    - Here, the code is handling a specific case where certain values in the logits are marked with -100. Typically, -100 is used as a masking value in NLP tasks to ignore specific tokens in loss calculations, such as padded tokens or special tokens that should not contribute to the model's learning.\n",
        "    - This line replaces all instances of -100 in the logits with the end-of-sequence (EOS) token ID from the tokenizer. The EOS token is used to signify the end of a text sequence. This replacement ensures that when converting logits back to text, the sequence ends appropriately at the designated end-of-sequence markers instead of continuing with tokens corresponding to -100, which are effectively placeholder or non-token values.\n",
        "\n",
        "3. `text_result = []`:\n",
        "    - This line initializes an empty list, `text_result`, which will be used to store the final generated text sequences after converting the logits (or token IDs) back into human-readable text.\n",
        "\n",
        "---\n",
        "\n",
        "This code is typically part of a larger process where the model's predictions (logits) are converted into text sequences. The logits are first cleaned or adjusted as necessary (e.g., replacing placeholder values with EOS token IDs), and then each sequence of logits is decoded into text, usually involving additional steps not shown here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:14:29.502838Z",
          "iopub.status.busy": "2024-03-10T12:14:29.502550Z",
          "iopub.status.idle": "2024-03-10T12:14:59.593419Z",
          "shell.execute_reply": "2024-03-10T12:14:59.592510Z",
          "shell.execute_reply.started": "2024-03-10T12:14:29.502813Z"
        },
        "id": "876i9Z9KG91F",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "eval_result = trainer.predict(test_dataset, max_new_tokens=5)\n",
        "logits = eval_result.predictions\n",
        "logits[logits == -100] = tokenizer.eos_token_id\n",
        "text_result = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGaLbAUSHDgf"
      },
      "source": [
        "# Task5: Decoding Model Predictions\n",
        "\n",
        "In this task, you will write code to decode the raw text predictions from your NLP model and process them to extract meaningful output. You will work with the logits generated by the model after evaluation and use them to produce human-readable text. Follow the instructions below to complete the missing parts of the code.\n",
        "\n",
        "#### Objectives:\n",
        "1. Decode raw model predictions into text.\n",
        "2. Process the decoded text to extract and clean the answers.\n",
        "\n",
        "#### Task Instructions:\n",
        "\n",
        "1. **Decode the Logits**:\n",
        "   - Use the `tokenizer.batch_decode` method to convert the `logits` obtained from model predictions into raw text sequences. Assign the result to `raw_text_result`.\n",
        "   - This method converts token IDs back into strings, making them human-readable.\n",
        "\n",
        "2. **Extract Context for Evaluation**:\n",
        "   - Retrieve the original questions (context) from `test_dataset` for reference or comparison. Store these in a list called `context` by iterating over the dataset and accessing the 'input' field for each item.\n",
        "\n",
        "3. **Process Decoded Text**:\n",
        "   - Initialize an empty list `text_result` to store the processed answers.\n",
        "   - Iterate over each decoded text sequence in `raw_text_result`:\n",
        "       - Remove any padding tokens from the text using the `replace` method.\n",
        "       - Find the index of the keyword 'Answer:' which separates the question from the answer in the decoded text.\n",
        "       - Extract the answer text by slicing the string from just after 'Answer:' to the end of the text.\n",
        "       - Check if the EOS (end-of-sequence) token exists in the answer and truncate the answer at this token to remove any trailing text.\n",
        "       - Append the cleaned answer to `text_result`.\n",
        "\n",
        "4. **Extract Ground Truth for Comparison**:\n",
        "   - Similar to extracting the context, retrieve the ground truth answers from `test_dataset` by accessing the 'target' field for each item. Store these in a list called `ground_truth`.\n",
        "\n",
        "5. **Output Results**:\n",
        "   - Print the first 10 entries of `text_result` to verify the decoding and processing steps.\n",
        "\n",
        "#### Usage:\n",
        "After completing the above steps, your code will effectively process the raw predictions from your NLP model into a clean, human-readable format, suitable for evaluation against the ground truth answers.\n",
        "\n",
        "```python\n",
        "# Place your completed code here based on the above instructions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:32:43.246181Z",
          "iopub.status.busy": "2024-03-10T12:32:43.245778Z",
          "iopub.status.idle": "2024-03-10T12:32:43.258630Z",
          "shell.execute_reply": "2024-03-10T12:32:43.257537Z",
          "shell.execute_reply.started": "2024-03-10T12:32:43.246150Z"
        },
        "id": "xbZJ7hhtHkkv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Your code here\n",
        "\n",
        "### End of Your code\n",
        "ground_truth = [test_dataset.__getitem__(i)['target'] for i in range(test_dataset.__len__())]\n",
        "for question, pred, gt in list(zip(context, text_result, ground_truth))[:10]:\n",
        "    print(f\"\"\"\n",
        "    Question: {question}\n",
        "    Prediction: {pred}\n",
        "    Ground Truth: {gt}\n",
        "    \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE4PQcoiIAti"
      },
      "source": [
        "# Calculating Evaluation Metrics\n",
        "\n",
        "The provided code block is involved in evaluating the performance of an NLP model, specifically focusing on text generation tasks such as translation, summarization, or question-answering. It calculates two common metrics used in NLP: BLEU and ROUGE. Here is what each part of the code is responsible for:\n",
        "\n",
        "1. `bleu_score = bleu_scorer.compute(predictions=text_result, references=ground_truth)`:\n",
        "    - This line computes the BLEU (Bilingual Evaluation Understudy) score for the model's predictions against the ground truth answers. BLEU measures the correspondence between the machine-generated text and one or more reference texts. It does this by comparing the presence and frequency of phrases in the generated text to those in the reference text(s), effectively quantifying the quality of the generated text. High BLEU scores indicate better matching with the reference, suggesting better translation or summarization quality.\n",
        "\n",
        "2. `rouge_score = rouge_scorer.compute(predictions=text_result, references=ground_truth)`:\n",
        "    - This line computes the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score. ROUGE is used primarily to evaluate text summarization quality but can also be applied to other generation tasks. Unlike BLEU, which is precision-oriented, ROUGE focuses on recall — it measures the amount of overlap (in terms of n-grams, word sequences, and word pairs) between the generated text and the reference texts. There are different variations of ROUGE, each focusing on different aspects of the texts' overlap. A higher ROUGE score indicates that more elements of the reference texts were captured in the generated text.\n",
        "\n",
        "3. `print(bleu_score)`, `print(rouge_score)`:\n",
        "    - These lines output the calculated BLEU and ROUGE scores, respectively. These scores give an indication of how well the generated text matches the expected output according to different metrics of textual similarity and quality.\n",
        "\n",
        "---\n",
        "\n",
        "By using these metrics, developers and researchers can quantitatively assess the performance of their NLP models in tasks like translation, summarization, or question answering. It helps in understanding the model's capabilities and areas for improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:31:33.273634Z",
          "iopub.status.busy": "2024-03-10T12:31:33.273222Z",
          "iopub.status.idle": "2024-03-10T12:31:33.529501Z",
          "shell.execute_reply": "2024-03-10T12:31:33.528352Z",
          "shell.execute_reply.started": "2024-03-10T12:31:33.273599Z"
        },
        "id": "yxo11uWjH_So",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bleu_score = bleu_scorer.compute(predictions=text_result, references=ground_truth)\n",
        "rouge_score = rouge_scorer.compute(predictions=text_result, references=ground_truth)\n",
        "print('BLEU1:', bleu_score['precisions'][0]*100)\n",
        "print(f\"\"\"\n",
        "ROUGE-1: {rouge_score['rouge1']*100}\n",
        "ROUGE-2: {rouge_score['rouge2']*100}\n",
        "ROUGE-L: {rouge_score['rougeL']*100}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZJzXLYvHt18"
      },
      "source": [
        "# Task6: Improve task performance & play with the model\n",
        "- From the last implementation, you can easily find the\n",
        "- You can improve the task performance by trying different model\n",
        "- You can play with the model, to give it some questions you like, and check what results can be outputted."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30665,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
